{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "As a toy example, say that we are interested\n",
    "in differentiating the function\n",
    "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to the column vector $\\mathbf{x}$.\n",
    "To start, let us create the variable `x` and assign it an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4.0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `requires_grad=True` argument when creating `x`, it tells the framework\n",
    "we need allocate gradient space for `x` in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # Is se to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `x` is a tensor of length 4,\n",
    "the `dot` operator will perform an inner product of `x` and `x`,\n",
    "yielding the scalar output that we assign to `y`.\n",
    "Next, we can automatically calculate the gradient of `y`\n",
    "with respect to each component of `x`\n",
    "by calling `y`'s `backward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we recheck the value of `x.grad`, we will find its contents overwritten by the newly calculated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function $y = 2\\mathbf{x}^{T}\\mathbf{x}$ with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$. Let us quickly verify that our desired gradient was calculated correctly. If the two tensors are indeed the same, then the equality between them holds at every position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we subsequently compute the gradient of another variable whose value was calculated as a function of x, we need to clear the previous values in `x.grad` first, as Pytorch accumulates and adds the gradient in default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clean the gradient \n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward for Non-Scalar Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad can be implicitly created only for scalar outputs with pytorch, you will get an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d2e3411165d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Clean the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d2l/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d2l/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d2l/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x.grad.zero_() # Clean the gradient\n",
    "y = x * x\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can pass a head grad to compute non-escalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 2., 4., 6.]), tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clean the gradient\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones_like(x))\n",
    "x.grad, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is the same that we sum the y results and do the backwards step, without passing the head gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 2., 4., 6.]), tensor(14., grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clean the gradient\n",
    "y = sum(x * x)\n",
    "y.backward()\n",
    "x.grad, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaching Computation\n",
    "\n",
    "Sometimes, we wish to move some calculations\n",
    "outside of the recorded computational graph.\n",
    "For example, say that `y` was calculated as a function of `x`,\n",
    "and that subsequently `z` was calculated as a function of both `y` and `x`.\n",
    "Now, imagine that we wanted to calculate\n",
    "the gradient of `z` with respect to `x`,\n",
    "but wanted for some reason to treat `y` as a constant,\n",
    "and only take into account the role\n",
    "that `x` played after `y` was calculated.\n",
    "\n",
    "Here, we can call `u = y.detach()` to return a new variable `u`\n",
    "that has the same value as `y` but discards any information\n",
    "about how `y` was computed in the computational graph.\n",
    "In other words, the gradient will not flow backwards through `u` to `x`.\n",
    "Yielding a `u` that will be treated as a constant in any `backward` call.\n",
    "Thus, the following `backward` function computes\n",
    "the partial derivative of `z = u * x` with respect to `x` while treating `u` as a constant,\n",
    "instead of the partial derivative of `z = x * x * x` with respect to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clean the gradient\n",
    "y = x * x\n",
    "u = y.detach() # Trait u as a constant\n",
    "z = u * x # z = u * x\n",
    "\n",
    "z.sum().backward() # dz/dx = u (We sum because u is not scalar)\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the computation of `y` was recorded,\n",
    "we can subsequently call `y.backward()` to get the derivative of `y = x * x` with respect to `x`, which is `2 * x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clean the gradient\n",
    "y.sum().backward() # Compute dy/dx = 2x (We sum because y is not scalar value)\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Gradient of Python Control Flow\n",
    "\n",
    "One benefit of using automatic differentiation\n",
    "is that even if building the computational graph of a function\n",
    "required passing through a maze of Python control flow\n",
    "(e.g., conditionals, loops, and arbitrary function calls),\n",
    "we can still calculate the gradient of the resulting variable.\n",
    "In the following snippet, note that\n",
    "the number of iterations of the `while` loop\n",
    "and the evaluation of the `if` statement\n",
    "both depend on the value of the input `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm().item() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum().item() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to compute gradients, we just need to record the calculation and then call the backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(1, ), requires_grad=True)\n",
    "d = f(a) # a bunch of multiplications on a d = k * a\n",
    "d.backward() # calculate df/da = k => f/a = df/da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now analyze the `f` function defined above.\n",
    "Note that it is piecewise linear in its input `a`.\n",
    "In other words, for any `a` there exists some constant scalar `k`\n",
    "such that `f(a) = k * a`, where the value of `k` depends on the input `a`.\n",
    "Consequently `d / a` allows us to verify that the gradient is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode and Prediction Mode\n",
    "\n",
    "When we get to complicated deep learning models,\n",
    "we will encounter some algorithms where the model\n",
    "behaves differently during training and\n",
    "when we subsequently use it to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we can set `model.train()` or `model.eval()` to distinguish the\n",
    "training mode and prediction mode, which we'll cover in the later sections of\n",
    "the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Deep learning frameworks can automate the calculation of derivatives. To use it, we first attach gradients to those variables with respect to which we desire partial derivatives. We then record the computation of our target value, execute its `backward` function, and access the resulting gradient via our variable's `grad` attribute.\n",
    "* We can detach gradients to control the part of the computation that will be used in the `backward` function.\n",
    "* The running modes include training mode and prediction mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
    "\n",
    "The second derivative is much more expensive because is build on top of the first derivative, so we need to build the first derivative computational graph and then calculate the second derivative on top of that graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After running `y.backward()`, immediately run it again and see what happens.\n",
    "\n",
    "It's calculate the same derivative, we need to pass an arguments telling pytorch to retain_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.])\n",
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0, requires_grad = True)\n",
    "y = x * x\n",
    "y.sum().backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or matrix. At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
    "\n",
    "The results is the constant `k` replicate in a tensor of shape equal to `a`, because `f(a) = k * a` then the derivative of $\\frac{df}{da} = k = \\frac{f(a)}{a}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(2,3), requires_grad=True)\n",
    "d = f(a) # d = k * a\n",
    "d.sum().backward() # calculate backward to the sum of non-scalars\n",
    "                    # dd/da = k => then d/a = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0519, -0.5885,  0.0626],\n",
       "         [ 0.1675,  1.7026,  1.4531]], requires_grad=True),\n",
       " tensor([[ -26.5704, -301.3363,   32.0743],\n",
       "         [  85.7696,  871.7515,  743.9756]], grad_fn=<MulBackward0>),\n",
       " tensor([[512., 512., 512.],\n",
       "         [512., 512., 512.]]),\n",
       " tensor([[512., 512., 512.],\n",
       "         [512., 512., 512.]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, d, a.grad, d/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Redesign an example of finding the gradient of the control flow. Run and analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    if a.item() > 100:\n",
    "        d = a/10\n",
    "    while d.item() > 1:\n",
    "        d /= 10\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(200, dtype=torch.float, requires_grad=True)\n",
    "d = f(a) # f = k * a => f/a = k\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2000, grad_fn=<DivBackward0>),\n",
       " tensor(200., requires_grad=True),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, a, a.grad, d/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Let $f(x) = \\sin(x)$. Plot $f(x)$ and $\\frac{df(x)}{dx}$, where the latter is computed without exploiting that $f'(x) = \\cos(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: torch.sin(x)\n",
    "df = lambda x: torch.cos(x)\n",
    "x = torch.linspace(-5, 5, steps = 200, requires_grad=True)\n",
    "sin = f(x)\n",
    "sin.sum().backward()\n",
    "cos = df(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0b9bef1fe840929a072cd313582c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x.detach(), sin.detach())\n",
    "plt.title(\"sin(x)\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x.detach(), cos.detach())\n",
    "plt.title(\"cos(x)\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x.detach(), x.grad)\n",
    "plt.title(\"cos(x) autograd with pytorch of sin(x)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "094cf61dbf554245903a16c227668c6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0dae99f20493452aa4d7b1e99ba27130": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "115653ce4894436aa9b339caa4dce5f8": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_c4b537b972ac4ed9a8ae26e8471175d0",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "1fbe70ae99734c4fa253268b48a11fb2": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_a4c2c6b6d80f481ea564f9c77fbd4d05",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "23cb67de7db7492f92fd1ea7420b9c03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "25f9c1ed6e9d46c8aa8c0c334d76951b": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_9bcc322d8c034da494bd2ae609ad441b",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "2bed8f20475a4ee581d0f06f2403b26a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5679fc45f2e84b3a904377d518180d23": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_edf6323cf7ae4f669cdbccaef151d39f",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "5a0b9bef1fe840929a072cd313582c41": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "MPLCanvasModel",
      "state": {
       "_cursor": "default",
       "_figure_label": "Figure 1",
       "_height": 500,
       "_image_mode": "diff",
       "_width": 800,
       "layout": "IPY_MODEL_d76e8c779a7a43839cd07f7374b716fd",
       "toolbar": "IPY_MODEL_25f9c1ed6e9d46c8aa8c0c334d76951b",
       "toolbar_position": "left"
      }
     },
     "72053c183b304464aa599c2054848285": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75641d4fb8dd4f3792c335f4035dd448": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9bcc322d8c034da494bd2ae609ad441b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a4c2c6b6d80f481ea564f9c77fbd4d05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6c8c6e321d44dfdbc6101b079a6b67d": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "MPLCanvasModel",
      "state": {
       "_cursor": "default",
       "_figure_label": "Figure 1",
       "_height": 500,
       "_image_mode": "diff",
       "_message": "x=-5.40814     y=0.189161    ",
       "_width": 800,
       "layout": "IPY_MODEL_094cf61dbf554245903a16c227668c6d",
       "toolbar": "IPY_MODEL_f8db096fb7884767b54e33484245d8e4",
       "toolbar_position": "left"
      }
     },
     "c4b537b972ac4ed9a8ae26e8471175d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c7a1831e873b437d8d43139ddf0e8888": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d76e8c779a7a43839cd07f7374b716fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e0861f0f009047e5afdb437897a2e38f": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_c7a1831e873b437d8d43139ddf0e8888",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "edf6323cf7ae4f669cdbccaef151d39f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8db096fb7884767b54e33484245d8e4": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.7.2",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_2bed8f20475a4ee581d0f06f2403b26a",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
